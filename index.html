<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SelfIE uses Large Language Models to interpret their own hidden embeddings.">
  <meta name="keywords" content="SelfIE, interpertability, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SelfIE: Self-Interpretation of Large Language Model Embeddings</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤³</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2310.10591">
            INVITE: INterpret Vision Transformer via Text Explanations
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">&#129331;<span class="dnerf" style="font-size: 120%">SelfIE</span>: Self-Interpretation of Large Language Model Embeddings</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tonychen.xyz/">Haozhe Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="http://www.cs.columbia.edu/~mcz/">Chengzhi Mao</a><sup>123</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Columbia University,</span>
            <span class="author-block"><sup>2</sup>Mila,</span>
            <span class="author-block"><sup>3</sup>McGill University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tonychenxyz/selfie"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="teaser-and-method">
  <div class="container is-max-desktop">
    <div class="teaser">
      <h2 class="teaser-text">
        <b><span class="dnerf">SelfIE</span> interprets each hidden embedding vector in a transformer-based LLM with natural language descriptions. </b> We can use <span class="dnerf">SelfIE</span> to understand how an LLM arrive at its answer internally. 
      </h2>
          <figure>
            <img src="static/images/real-teaser.svg" alt="SelfIE interprets hidden embedding with text descriptions">
            <figcaption>SelfIE interprets hidden embedding in LLMs with text descriptions.</figcaption>
          </figure>
          
    </div>
</section>
<section class="safety-example-top">
  <div class="container is-max-desktop">
    <div class="safety-problem-top">
      <h2 class="teaser-text">
        <b>Example</b>: We used <span class="dnerf">SelfIE</span> to detect harmful knowledge inside LLM and attain deep alignment by erasing the knowledge.
      </h2>
    </div>
    <div class = "safety-top-image">
      <figure>
      <img src="static/images/safety-solved-all-v3.svg" alt="SelfIE helps understand safety issue inside LLM">
      <figcaption>We used <span class="dnerf">SelfIE</span> to detect and remove harmful knowledge in LLMs.</figcaption>
    </figure>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content ">
          <p>
            The expanding impacts of Large Language Models (LLMs) increasingly require the answer to: How do LLMs obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose <span class="dnerf">SelfIE</span> (Self-Interpretation of Embeddings) that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, <span class="dnerf">SelfIE</span> reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. <span class="dnerf">SelfIE</span>'s text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets. 
          </p>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">How does  <span class="dnerf">SelfIE</span> work?</h2>
          <div class="content ">
            <div class = "surgery-image">
              <figure>
              <img src="static/images/surgery-final.svg" alt="SelfIE interprets LLM hidden embedding with a modified interpretation forward pass">
              <figcaption><span class="dnerf">SelfIE</span> interprets LLM hidden embedding with a modified interpretation forward pass.</figcaption>
            </figure>
            </div>
            <ol>
              <li>Extract hidden embedding to interpret from an original forward pass of an input prompt through LLM. </li>
              <li>Interpret the embedding with interpretation forward pass through the same LLM.</li>
              <li>The interpretation forward pass takes in an interpretation prompt to summarize the embedding. </li>
              <li>Inject the embedding to interpret in the placeholder of the interpretation prompt during interpretation forward pass.</li>
            </ol>
            <p>We also calculate a Relevancy Score as treatment effect of injecting embedding to intererpt in interpretation forward pass to measure how relevant the interpretation is to the embedding being interpreted. Relevancy Score will be shown in <span class = "relevancy">highlight</span> in examples below.</p>
          </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Understand LLM reasoning with <span class="dnerf">SelfIE</span></h2>
            <div class="content ">
              <p> While previous interpretation methods such as linear probes can only interpret a closed set of concepts with training, <span class="dnerf">SelfIE</span> can interpret open-world concepts without any training. We therefore could use <span class="dnerf">SelfIE</span> to understand LLM internal reasoning in general. </p>
              
              <div class = "qualitative-understanding-images">
                <figure>
                <img src="static/images/moral-web.svg" alt="Understand LLM reasoning when explanation changes answer with SelfIE">
                <figcaption> Using <span class="dnerf">SelfIE</span> to access reasoning when demanding explanation changes LLM response.</figcaption>
              </figure>
              </div>
              <p><b>Access Reasoning When Explanation Changes Response.</b> When LLaMA is asked about making a decision in the trolley problem scenario, attaching <em>explain reason.</em> at the end of the prompt alters LLaMA's answer. Therefore, we cannot access LLaMA's reasoning to the answer <em>Yes</em> when asked to answer in only one word from the output. <span class="dnerf">SelfIE</span> reveals that the answer <em>Yes</em> might be result of conforming to majority opinions.</p>
              
              <div class = "qualitative-understanding-images">
                <figure>
                <img src="static/images/prompt_injection_web.svg" alt="Understand why prompt injection works with SelfIE">
                <figcaption>Using <span class="dnerf">SelfIE</span> to understand why prompt injections work.</figcaption>
              </figure>
              </div>
              <p><b>Why Prompt Injection Works.</b> We use <span class="dnerf">SelfIE</span> to understand why prompt injections steer LLaMA to provide harmful answers. <span class="dnerf">SelfIE</span> reveals that the model concludes urgency from the exclamation mark in the early layer and infers user is in crisis in the late layers, before finally complying with harmful requests to avoid user aggression. </p>
              <div class = "qualitative-understanding-images">
                <figure>
                <img src="static/images/physics-web.svg" alt="Understand how LLM reasons through a physics question with SelfIE">
                <figcaption>Using <span class="dnerf">SelfIE</span> to understand how LLaMA reasons through a physics question that requires advanced knowledge.</figcaption>
              </figure>
              </div>
              <p><b>Reasoning with Knowledge.</b> We use <span class="dnerf">SelfIE</span> to examine how LLaMA answers a physics reasoning question. We found that the model extracts the <em>glittery.</em> aspect of <em>syrup.</em> in early layers, grasps <em>thickness</em> as the relevant quality, and retrieves the advanced physics concept <em>viscosity.</em> that is related to thickness.  </p>
              <div class = "qualitative-understanding-images">
                <figure>
                <img src="static/images/hallucination.svg" alt="Understand how LLM hallucinates with SelfIE">
                <figcaption>Using <span class="dnerf">SelfIE</span> to understand how LLaMA hallucinates when responding to a question involving a fictitious name.
                </figcaption>
              </figure>
              </div>
              <p><b>How hallucination occurs.</b> We use <span class="dnerf">SelfIE</span> to trace how LLaMA hallucinates when responding to a question involving a fictitious name. LLaMA first recalls <em>Mc</em> as in <em>McDonalds</em> in <em>Scotland</em> and associate <em>McQueen</em> with <em>Scotland</em>. It then associate <em>Mc</em> and <em>Scotland</em> with a similar name <em>McLean</em> who is a doctor. It finally combines the information about <em>McLean</em> as a doctor back to <em>McQueen</em> and produces final understanding of <em>McQueen</em> as a researcher in psychiatry.   </p>
              <div class = "qualitative-understanding-images">
                <figure>
                <img src="static/images/social-reasoning.svg" alt="Understand how LLM approaches social reasoning with SelfIE">
                <figcaption>Using <span class="dnerf">SelfIE</span> to understand how LLaMA makes inference about human social interactions in a nuanced social situation.
                </figcaption>
              </figure>
              </div>
              <p><b>Social reasoning.</b> We use <span class="dnerf">SelfIE</span> to reveal how LLaMA approaches a complex social scenario. We showed that LLaMA is able to infer mental states and intentions of different parties in a social situation and formulate the final output with these understandings. <a href ="static/examples/social_reasoning.html"> View interpretations of all hidden embeddings in model</a>. </p>
            </div>
          </div>
        </div>


        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Control LLM reasoning with <span class="dnerf">SelfIE</span></h2>
              <div class="content ">
                <p> The text interpretation produced by <span class="dnerf">SelfIE</span> enables new modes intervening on model weights and control model reasoning behaviors. We propose two control methods based on <span class="dnerf">SelfIE</span> interpretation: <b>Supervised Control</b> and <b>Reinforcement Control</b>. </p>
                <p>Compared to previous model editing methods,</p>
                  <ol>
                     <li>Our methods only requires gradient calculation of individual layer instead of entire model. Our methods therefore scale more efficiently to large models.</li>
                    <li>Our methods can edit open-ended concepts beyond simple facts.</li>
                    <li>Since control can be done on single layer and single/few samples, our methods are fast. Each model behavior change takes 10 seconds to 2 minutes on a 70B model. </li>
                    <li>Reinforcement Control does not require supervised target.</li>  
                    
                  </ol>
                  <h3>Supervised Control.</h3>
                  <p>Supervised Control modifies model weights so that a layer produces embeddings that interpret to some target interpretation.</p>
                  <figure>
                  <img src="static/images/frozen-sc.svg" alt="Pipeline for supervised control">
                  <figcaption>Pipeline for Supervised Control. Supervised Control (1) isolates one layer; (2) identify an input embedding and a target embedding that interprets to target interpretation; (3) modifies the layer's weight to output the target embedding. </figcaption>
                </figure>
                  <p><b>Example: Change LLM's perception of an open-ended concept.</b> We applied Supervised Control on one layer so that LLM understands molotov cocktail as a drink. LLM is able to generalize this new understanding to complex reasoning that requires indirect understanding of molotov cocktail's nature. We updated the model parameter eight times with gradient descent, where each update takes 10 seconds.</p>
                  <figure>
                  <div class="control-example-images"><img src="static/images/molotov-editing-final.svg" alt="Supervised control example"></div>
                  <figcaption>Supervised Control based on <span class="dnerf">SelfIE</span> modifies LLM's perception of molotov cocktail to a drink.</figcaption>
                </figure>
                  <p><b>Example: Overriding ethical preference in user prompt.</b> LLMs are susceptible to being steered to undesirable ethical ideas with user specification of moral beliefs in a prompt. We used Supervise Control to override user's specification of prioritizing humans over aliens in a hypothetical scenario. The control generalizes to unseen prompts, and we show the result is not memorization since LLM integrates the edited concept into coherent reasoning. We updated the model parameter twice with gradient descent that only takes 20 seconds.</p>
                  <div class="control-example-images"><img src="static/images/moral-editing.svg" alt="Supervised control example"></div>
                  <figcaption>Supervised Control based on <span class="dnerf">SelfIE</span> overrides user's specification of prioritizing humans over aliens in a hypothetical scenario.</figcaption>
                  <h3>Reinforcement Control.</h3>
                  <p>Previous works control LLM reasoning from output level with methods such as RLHF without needing supervised targets. We extend this class of method to produce reward signals by evaluating embedding interpretation text and adjust layer parameter to maximize reward. </p>
                  <figure>
                  <img src="static/images/forzen-rcv2.svg" alt="RC pipeline">
                  <figcaption>Pipeline for Reinforcement Control. Reinforcement Control (1) isolates one layer; (2) evaluate the interpretations of embeddings outputted by the layer with humans or machines; (3) modifies the layer weight based on reward given by humans or machines. </figcaption>
                </figure>
                  <p><b>Example: Erase harmful knowledge in LLM.</b> We used Reinforcement Control to erase harmful knowledge in LLM without supervision targets. We prompt an evaluator LLM to evaluate whether an embedding interpretation text contains harmful information and give positive/negative rewards on non-harmful/harmful information. We conduct control with regular, non-prompt-injection prompt, and the control result generalizes to refuse providing harmful information in unseen prompt injection. We found that the model control also refuses to answer questions about other unrelated criminal behaviors in prompt injection. We applied eight parameter updates with gradient descent on layer 15, where each update only takes 10 seconds.</p>
                  <div class="safety-solved-detail-image">
                    <figure>
                    <img src="static/images/rc-example-boldv2.svg" alt="Reinforcement control example">
                    <figcaption>Reinforcement Control based on <span class="dnerf">SelfIE</span> erases harmful knowledge in LLM without supervision targets.</figcaption>
                    </figure>
                  </div>

              </div>
            </div>
          </div>


        
    

    <!--/ Abstract. -->

    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content ">
          <p>
            <b>LLM model interpretation</b><br><br>
            Interpret LLM through probing
            <ul>
              <li> <a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a></li>
              <li><a href="https://arxiv.org/abs/2106.00737">Implicit Representations of Meaning in Neural Language Models</a></li>
            </ul>
            Mechanistic decomposition of transformer
            <ul>
              <li><a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</a></li>
            </ul>
            Interpret LLM through decoding next tokens
            <ul>
              <li><a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">interpreting GPT: the logit lens</a></li>
              <li><a href="https://arxiv.org/abs/2303.08112">Eliciting Latent Predictions from Transformers with the Tuned Lens</a></li>
              <li><a href="https://arxiv.org/abs/2311.04897">Future Lens: Anticipating Subsequent Tokens from a Single Hidden State</a></li>
              <li><a href="https://arxiv.org/pdf/2203.14680.pdf">Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space</a></li>
              <li><a href="https://arxiv.org/pdf/2308.09124.pdf">Linearity of Relation Decoding in Transformer Language Models</a></li>
              <li><a href="https://arxiv.org/pdf/2204.12130.pdf">LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models</a></li>
              <li><a href="https://arxiv.org/pdf/2012.14913.pdf">Transformer Feed-Forward Layers Are Key-Value Memories</a></li>
            </ul>
            Concurrent work of interpreting open-world concepts in LLM 
            <ul>
              <li><a href ="https://arxiv.org/abs/2401.06102">Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models</a>
            </ul>
            Hypothesis of hierachial reasoning inside of LLM
            <ul>
              <li><a href="https://transformer-circuits.pub/2023/july-update/index.html#safety-features">What would be the most safety-relevant features in Language Models?</a></li>
            </ul>
          </p>
          <p>
            <b>LLM model editing</b><br>
            <ul>
              <li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a></li>
              <li><a href="https://arxiv.org/abs/2110.11309">Fast Model Editing at Scale</a></li>
              <li><a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a></li>
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
    
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Webpage template comes from <a
            href="https://github.com/nerfies/nerfies.github.io">this project</a> and <a
            href="https://viper.cs.columbia.edu/">this project</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
